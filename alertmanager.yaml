---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: alertmanager
---

apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-alertmanager-config
  labels:
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/component: alertmanager
data:
  # To learn more: https://prometheus.io/docs/alerting/configuration/
  alertmanager.yml: |
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 10s
      receiver: 'email'
    receivers:
    - name: 'email'
      email_configs:
      - to: '<mail id>'
        from: '<mail id>'
        smarthost: mails.in:587
        auth_username: '<mail id>'
        auth_password: '*****'
    inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'dev', 'instance']
---
apiVersion: v1
kind: Service
metadata:
  name: monitoring-alertmanager-operated
  labels:
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/component: alertmanager
spec:
  type: "ClusterIP"
  clusterIP: None
  selector:
    k8s-app: alertmanager
  ports:
    - name: mesh
      # Exposes port 6783 at a cluster-internal IP address
      port: 6783
      protocol: TCP
      # Routes requests to port 6783 of the Alertmanager StatefulSet Pods
      targetPort: 6783
    - name: http
      port: 9093
      protocol: TCP
      targetPort: 9093
---
apiVersion: v1
kind: Service
metadata:
  name: monitoring-alertmanager
  labels:
    k8s-app: alertmanager
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/component: alertmanager
spec:
  ports:
    - name: http
      # Exposes port 9093 at a cluster-internal IP address
      port: 9093
      protocol: TCP
      # Routes requests to port 9093 of the Alertmanager StatefulSet Pods
      targetPort: 9093
  selector:
    k8s-app: alertmanager
  type: "ClusterIP"
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: monitoring-alertmanager
  labels: &Labels
    k8s-app: alertmanager
    app.kubernetes.io/name: monitoring
    app.kubernetes.io/component: alertmanager
spec:
  # Used to configure a stable network identity for the StatefulSet Pods 
  # e.g. pod_name.monitoring-alertmanager-operated.monitoring.svc:6783
  serviceName: "monitoring-alertmanager-operated"
  # To scale the number of Alertmanager replicas, you must add `â€”cluster.peer` addresses 
  # for any additional Alertmanager Pods in the `args` section of the `containers` spec. 
  replicas: 1
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  selector:
    matchLabels: *Labels
  template:
    metadata:
      labels: *Labels
    spec:
      serviceAccountName: alertmanager
      containers:
        - name: prometheus-alertmanager
          # The Alertmanager container image
          image: quay.io/prometheus/alertmanager:v0.16.0
          imagePullPolicy: Always
          args:
            - --config.file=/etc/config/alertmanager.yml
            - --storage.path=/data
            - --web.listen-address=:9093
            - --web.route-prefix=/
            - --cluster.listen-address=$(POD_IP):6783
            - --cluster.peer=monitoring-alertmanager-0.monitoring-alertmanager-operated.monitoring.svc:6783
            - --cluster.peer=monitoring-alertmanager-1.monitoring-alertmanager-operated.monitoring.svc:6783
            - --log.level=debug
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          ports:
            # Configured for web access
            - containerPort: 9093
              name: http
            # Configured for communication over the mesh
            - containerPort: 6783
              name: mesh
          readinessProbe:
            httpGet:
              path: /#/status
              port: 9093
            initialDelaySeconds: 30
            timeoutSeconds: 30
          volumeMounts:
            # The ConfigMap containing `alertmanager.yml` will be mounted to `/etc/config`, using a Kubernetes Volume. 
            # To learn more about mounting ConfigMap data into a Volume, 
            # consult https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#populate-a-volume-with-data-stored-in-a-configmap
            - name: config-volume
              mountPath: /etc/config
            - name: monitoring-alertmanager-data
              mountPath: "/data"
              subPath: ""
          resources:
            # Resource limits of 50 MiB of memory and 10m of CPU. 
            # To learn more about resource limits and requests, and how to tune them, 
            # consult https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container
            limits:
              cpu: 10m
              memory: 50Mi
            requests:
              cpu: 10m
              memory: 50Mi
      volumes:
        - name: config-volume
          configMap:
            name: monitoring-alertmanager-config
      # Configures Pod anti-affinity so that Alertmanager Pods are assigned to different Nodes. 
      # To learn more about Pod affinity and anti-affinity, consult https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - alertmanager
            topologyKey: "kubernetes.io/hostname"
  # Attach `2Gi` of DigitalOcean Block Storage to each Alertmanager Pod. 
  # This volume will be attached at the `/data` path and will be used to store Alertmanager data.
  # To learn more about `volumeClaimTemplate`, consult https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-storage
  volumeClaimTemplates:
  - metadata:
      name: monitoring-alertmanager-data
    spec:
      storageClassName: hcloud-volumes
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: "2Gi"